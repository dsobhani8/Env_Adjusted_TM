{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsobhani8/Env_Adjusted_TM/blob/main/model_iii_global_theta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-pdbnl7X3X",
        "outputId": "1505ead8-060a-433f-9d4d-06774878a842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Env_Adjusted_TM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dsobhani8/Env_Adjusted_TM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlhkhCeBVIIL",
        "outputId": "687d2dc9-d8ac-4c17-9017-c534acfd50f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions as dist\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.distributions import Normal, Distribution, HalfCauchy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iKxpTIBWxuQQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first reads a CSV file containing combined data, then splits it into a training set and a test set. Next, it tokenizes and vectorizes the 'text' column from the training and test data. To upload your personal dataset, replace the path in the pd.read_csv function with the path to your CSV file. Ensure the environment information exists in a column named 'source'.\n",
        "The environment index is acquired by mapping the unique values in the 'source' column of the training data to unique integer indices. For example, if the sources were 'Fox', 'NBC', and 'CBS', they would be represented as 0, 1, and 2 respectively."
      ],
      "metadata": {
        "id": "NVG2DcTPecDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A4ebiLxw7i64"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/Env_Adjusted_TM/data/zurich_stopwords.txt\"\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    stopwords_list = file.readlines()\n",
        "\n",
        "all_stopwords = [word.strip() for word in stopwords_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GKibgGW97j9V"
      },
      "outputs": [],
      "source": [
        "class LemmaTokenizer:\n",
        "\tdef __init__(self):\n",
        "\t\tself.wnl = WordNetLemmatizer()\n",
        "\tdef __call__(self, doc):\n",
        "\t\treturn [t for t in word_tokenize(doc) if str.isalpha(t)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your data goes here. It should be loaded as a dataframe with columns 'text' and 'source'"
      ],
      "metadata": {
        "id": "2_Xw8GXSrrKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = 'path/to/your/csvfile.csv'  # Replace with your CSV file path\n",
        "# combined_data = pd.read_csv(file_path)\n",
        "\n",
        "# # Splitting the data into training and testing sets\n",
        "# train_data = _\n",
        "# test_data = _"
      ],
      "metadata": {
        "id": "FSF5kTVIrjCY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC0bsas37pSQ",
        "outputId": "e7d29d56-10a5-47a2-fab7-502ce6b164d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['sha', 'words'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 1), stop_words=all_stopwords, max_df=0.4, min_df=0.0006)\n",
        "\n",
        "docs_word_matrix_raw = vectorizer.fit_transform(train_data['text'])\n",
        "test_data_word_matrix_raw = vectorizer.transform(test_data['text'])\n",
        "\n",
        "env_mapping = {value: index for index, value in enumerate(train_data['source'].unique())}\n",
        "env_index = train_data['source'].apply(lambda x: env_mapping[x])\n",
        "\n",
        "docs_word_matrix_tensor = torch.from_numpy(docs_word_matrix_raw.toarray()).float().to(device)\n",
        "env_index_tensor = torch.from_numpy(env_index.to_numpy()).long().to(device)\n",
        "test_data_word_matrix_tensor = torch.from_numpy(test_data_word_matrix_raw.toarray()).float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNhYXjEH8i96",
        "outputId": "b75b4298-5989-44fd-cad4-772dfb945019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16000, 3751])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(docs_word_matrix_tensor.shape)\n",
        "env_index.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code consists of two classes, Horseshoe and LDA. The Horseshoe class represents a horseshoe distribution. The Env_TM class is a topic model variant which models topics within a collection of documents and includes the parameters, priors, and a forward method to calculate the topic distribution for a given bag-of-words (BoW) representation and environment index."
      ],
      "metadata": {
        "id": "c-GQzR9DgyM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wCaQoRRBVGJm"
      },
      "outputs": [],
      "source": [
        "class Horseshoe(Distribution):\n",
        "    arg_constraints = {}\n",
        "\n",
        "    def __init__(self, scale, device):\n",
        "        self.scale = scale\n",
        "        self.device = device\n",
        "        self.half_cauchy = HalfCauchy(torch.tensor(scale, device=device))\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "      tau = self.half_cauchy.rsample(sample_shape)\n",
        "      return torch.abs(Normal(torch.zeros(sample_shape, device=self.device), tau.add(1e-8)).rsample())\n",
        "\n",
        "    def log_prob(self, value):\n",
        "        value = value.to(self.device)\n",
        "        tau = (value / self.scale).abs()\n",
        "        return self.half_cauchy.log_prob(tau) + Normal(torch.zeros_like(value, device=self.device), tau.add(1e-8)).log_prob(value)\n",
        "\n",
        "class EnvTM(nn.Module):\n",
        "    def __init__(self, num_topics, num_words, num_envs, device='cpu'):\n",
        "        super(EnvTM, self).__init__()\n",
        "\n",
        "        # Initialize parameters\n",
        "        def init_param(shape):\n",
        "            return nn.Parameter(torch.randn(shape, device=device))\n",
        "\n",
        "        def init_param_zeros(shape):\n",
        "            return nn.Parameter(torch.zeros(shape, device=device))\n",
        "\n",
        "        self.num_topics, self.num_words, self.num_envs = num_topics, num_words, num_envs\n",
        "\n",
        "        #Global Beta\n",
        "        self.beta = init_param([num_topics, num_words])\n",
        "        self.beta_logvar = init_param_zeros([num_topics, num_words])\n",
        "        self.beta_prior = Normal(torch.zeros([num_topics, num_words], device=device), torch.ones([num_topics, num_words], device=device))\n",
        "\n",
        "        #Gamma\n",
        "        self.gamma = init_param([num_envs, num_topics, num_words])\n",
        "        self.gamma_logvar = init_param_zeros([num_envs, num_topics, num_words])\n",
        "        self.gamma_prior = Horseshoe(scale=0.0001, device=device)\n",
        "\n",
        "        #Variational Parameters for beta_ek\n",
        "        self.beta_ek = init_param([num_envs, num_topics, num_words])\n",
        "        self.beta_ek_logvar = init_param_zeros([num_envs, num_topics, num_words])\n",
        "\n",
        "        # Global Theta\n",
        "        self.theta_global_prior = Normal(torch.zeros(num_topics, device=device), torch.ones(num_topics, device=device))\n",
        "\n",
        "        # Neural network for document-specific theta_global\n",
        "        self.theta_global_net = nn.Sequential(\n",
        "            nn.Linear(num_words, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, num_topics * 2)  # Produces both mean and variance\n",
        "        )\n",
        "\n",
        "        # Environment-specific theta parameters\n",
        "        self.theta_env_prior = Horseshoe(scale=0.01, device=device)\n",
        "        self.theta_env_mu = init_param([num_envs, num_topics])\n",
        "        self.theta_env_logvar = init_param_zeros([num_envs, num_topics])\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        eps = torch.randn_like(mu)\n",
        "        std = torch.exp(0.5 * logvar).add(1e-8)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, bow, env_index):\n",
        "        batch_size, vocab_size = bow.size()\n",
        "\n",
        "        # Get document-specific theta_global mean and variance\n",
        "        self.theta_global_params = self.theta_global_net(bow)\n",
        "        theta_global_mu, theta_global_logvar = self.theta_global_params.split(self.num_topics, dim=-1)\n",
        "        theta_global_logvar = theta_global_logvar.add(1e-8)\n",
        "\n",
        "        # Sample from Normal distribution\n",
        "        theta_global = Normal(theta_global_mu, torch.exp(0.5 * theta_global_logvar).add(1e-8)).rsample()\n",
        "\n",
        "        # Get environment-specific theta\n",
        "        theta_env_mu = self.theta_env_mu[env_index]\n",
        "        theta_env_logvar = self.theta_env_logvar[env_index]\n",
        "        theta_env = Normal(theta_env_mu, torch.exp(0.5 * theta_env_logvar).add(1e-8)).rsample()\n",
        "\n",
        "        # Combine theta_global and theta_env\n",
        "        theta = theta_global + theta_env\n",
        "        theta_softmax = F.softmax(theta, dim=-1)\n",
        "\n",
        "        beta_dist = Normal(self.beta, torch.exp(0.5 * self.beta_logvar).add(1e-8))\n",
        "        beta_sample = beta_dist.rsample()\n",
        "\n",
        "        gamma_dist = Normal(self.gamma[env_index], torch.exp(0.5 * self.gamma_logvar[env_index]).add(1e-8))\n",
        "        gamma_sample = gamma_dist.rsample()\n",
        "\n",
        "        beta_ek_mean = beta_sample + gamma_sample\n",
        "        beta_ek_dist = Normal(beta_ek_mean, torch.exp(0.5 * self.beta_ek_logvar[env_index]).add(1e-8))\n",
        "        beta_ek_sample = beta_ek_dist.rsample()\n",
        "\n",
        "        beta_gamma_ek_softmax = F.softmax(beta_ek_sample, dim=-1)\n",
        "\n",
        "        z = theta_softmax @ beta_gamma_ek_softmax\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0UHtk7TbWHrL"
      },
      "outputs": [],
      "source": [
        "def kl_divergence(p, q, num_samples=3800):\n",
        "    samples = p.rsample(sample_shape=(num_samples,))\n",
        "    return (p.log_prob(samples) - q.log_prob(samples)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_kl_divergences(EnvTM, env):\n",
        "    theta_global_mu, theta_global_logvar = EnvTM.theta_global_params.split(EnvTM.num_topics, dim=-1)\n",
        "    theta_global_logvar = theta_global_logvar.add(1e-8)\n",
        "    theta_global = Normal(theta_global_mu, torch.exp(0.5 * theta_global_logvar).add(1e-8))\n",
        "    theta_global_kl = torch.distributions.kl.kl_divergence(theta_global, EnvTM.theta_global_prior).sum()\n",
        "\n",
        "    theta_env_mu = EnvTM.theta_env_mu[env]\n",
        "    theta_env_logvar = EnvTM.theta_env_logvar[env]\n",
        "    theta_env = Normal(theta_env_mu, torch.exp(0.5 * theta_env_logvar).add(1e-8))\n",
        "    theta_env_kl = kl_divergence(theta_env, EnvTM.theta_env_prior).sum()\n",
        "\n",
        "    beta = Normal(EnvTM.beta, torch.exp(0.5 * EnvTM.beta_logvar))\n",
        "    beta_kl = torch.distributions.kl.kl_divergence(beta, EnvTM.beta_prior).sum()\n",
        "\n",
        "    beta_ek = Normal(EnvTM.beta_ek[env], torch.exp(0.5 * EnvTM.beta_ek_logvar[env]))\n",
        "    beta_ek_prior = Normal(EnvTM.beta.unsqueeze(0) + EnvTM.gamma[env], torch.ones_like(EnvTM.beta_ek[env]))\n",
        "    beta_ek_kl = torch.distributions.kl.kl_divergence(beta_ek, beta_ek_prior).sum()\n",
        "\n",
        "    gamma = Normal(EnvTM.gamma[env], torch.exp(0.5 * EnvTM.gamma_logvar[env]).add(1e-8))\n",
        "    gamma_kl = kl_divergence(gamma, EnvTM.gamma_prior)\n",
        "\n",
        "    return theta_global_kl, theta_env_kl, beta_kl, gamma_kl, beta_ek_kl"
      ],
      "metadata": {
        "id": "-8KTRruykdFI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bbvi_update(minibatch, env_index, EnvTM, optimizer, n_samples):\n",
        "    optimizer.zero_grad()\n",
        "    elbo_accumulator = torch.zeros(1, device=minibatch.device)  # Initialize as tensor\n",
        "\n",
        "    # Get the unique environment indexes\n",
        "    unique_envs = torch.unique(env_index)\n",
        "\n",
        "    for env in unique_envs:\n",
        "        # Get the mask where current environment matches\n",
        "        mask = (env_index == env)\n",
        "\n",
        "        # Use the mask to get the current minibatch and number of samples\n",
        "        current_minibatch = minibatch[mask]\n",
        "        current_n_samples = n_samples * mask.sum().item() / minibatch.size()[0]\n",
        "\n",
        "        # Pass the current minibatch to the model\n",
        "        z = EnvTM(current_minibatch, env)\n",
        "        theta_global_params = EnvTM.theta_global_net(current_minibatch)\n",
        "\n",
        "        theta_global_kl, theta_env_kl, beta_kl, gamma_kl, beta_ek_kl = calculate_kl_divergences(EnvTM, env)\n",
        "\n",
        "        elbo = (current_minibatch * z.log()).sum(-1).mul(current_n_samples).sub(theta_global_kl + theta_env_kl + beta_kl + gamma_kl + beta_ek_kl)\n",
        "        elbo_accumulator += elbo.sum()\n",
        "\n",
        "    (-elbo_accumulator).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return elbo_accumulator.item()"
      ],
      "metadata": {
        "id": "U9DZCIWDkeMV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameters such as num_topics, num_envs, num_epochs, minibatch_size, and the learning rate (lr) in this model are configurable based on the specific use case and data characteristics."
      ],
      "metadata": {
        "id": "j13Q_UsNm793"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(EnvTM, docs_word_matrix_tensor, env_index_tensor, num_epochs=80, minibatch_size=1024, lr=0.01):\n",
        "    EnvTM = EnvTM.to(device)  # move your model to the GPU\n",
        "    optimizer = torch.optim.Adam(EnvTM.parameters(), lr=lr)\n",
        "\n",
        "    docs_word_matrix_tensor = docs_word_matrix_tensor.to(device)  # move your tensors to the GPU\n",
        "    env_index_tensor = env_index_tensor.to(device)  # move your tensors to the GPU\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        elbo_accumulator = 0.0\n",
        "\n",
        "        # Randomly permute data for minibatches\n",
        "        permutation = torch.randperm(docs_word_matrix_tensor.size()[0])\n",
        "\n",
        "        for i in range(0, docs_word_matrix_tensor.size()[0], minibatch_size):\n",
        "            # Get minibatch\n",
        "            indices = permutation[i:i+minibatch_size]\n",
        "            minibatch = docs_word_matrix_tensor[indices]\n",
        "\n",
        "            # Get corresponding environment indices\n",
        "            minibatch_env_index = env_index_tensor[indices]\n",
        "\n",
        "            # BBVI update for the minibatch\n",
        "            elbo = bbvi_update(minibatch, minibatch_env_index, EnvTM, optimizer, docs_word_matrix_tensor.size()[0])\n",
        "\n",
        "            # Accumulate ELBO\n",
        "            elbo_accumulator += elbo\n",
        "\n",
        "        # Calculate average ELBO for the epoch\n",
        "        avg_elbo = elbo_accumulator / (docs_word_matrix_tensor.size()[0] / minibatch_size)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}, Average ELBO: {avg_elbo}')\n"
      ],
      "metadata": {
        "id": "q3VSPS2rmXHM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 6\n",
        "num_envs = 3\n",
        "env_tm_model = EnvTM(num_topics=num_topics, num_words=len(vectorizer.get_feature_names_out()), num_envs=num_envs, device=device)\n",
        "\n",
        "train_model(env_tm_model, docs_word_matrix_tensor, env_index_tensor, num_epochs=80, minibatch_size=1024, lr=0.01)"
      ],
      "metadata": {
        "id": "ytu3HIKUmZBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(env_tm_model, test_data_word_matrix_tensor):\n",
        "    # Move model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env_tm_model.to(device)\n",
        "    # Set the model to evaluation mode\n",
        "    env_tm_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        theta_test_params = env_tm_model.theta_global_net(test_data_word_matrix_tensor)\n",
        "        theta_test_mu, theta_test_logvar = theta_test_params.split(env_tm_model.num_topics, dim=-1)\n",
        "        theta_test_dist = Normal(theta_test_mu, torch.exp(0.5 * theta_test_logvar).add(1e-8))\n",
        "        theta_test = theta_test_dist.rsample()\n",
        "        theta_test_softmax = F.softmax(theta_test, dim=-1)\n",
        "        beta_test_softmax = F.softmax(env_tm_model.beta.to(device), dim=-1)\n",
        "        log_likelihood = torch.mm(theta_test_softmax, beta_test_softmax)\n",
        "        N = torch.sum(test_data_word_matrix_tensor)\n",
        "        log_perplex = -torch.sum(torch.log(log_likelihood) * test_data_word_matrix_tensor) / N\n",
        "        perplexity = torch.exp(log_perplex)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "perplexity = evaluate_model(env_tm_model, test_data_word_matrix_tensor)\n",
        "print(f'Test Perplexity: {perplexity}')"
      ],
      "metadata": {
        "id": "R00PBmJOoUKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEZNTHta3wie"
      },
      "outputs": [],
      "source": [
        "def print_top_words(env_tm_model, vectorizer, num_top_words):\n",
        "    beta = torch.nn.functional.softmax(env_tm_model.beta, dim=1)  # Convert to probabilities\n",
        "    beta = beta.cpu()  # Move to CPU\n",
        "    for i, topic in enumerate(beta):\n",
        "        top_words = topic.topk(num_top_words).indices\n",
        "        print(f'Topic {i+1}: {[vectorizer.get_feature_names_out()[i] for i in top_words]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKrLkg_L6r5y"
      },
      "outputs": [],
      "source": [
        "print_top_words(env_tm_model, vectorizer, num_top_words=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4hlunZOisQC"
      },
      "outputs": [],
      "source": [
        "def print_top_words(env_tm_model, vectorizer, num_top_words):\n",
        "    global_beta = torch.nn.functional.softmax(env_tm_model.beta, dim=1)  # Convert to probabilities\n",
        "    gamma = env_tm_model.gamma\n",
        "\n",
        "    # Print top words for global beta\n",
        "    print(\"Top words for global beta:\")\n",
        "    for i, topic in enumerate(global_beta):\n",
        "        top_words = topic.topk(num_top_words).indices\n",
        "        print(f'Topic {i+1}: {[vectorizer.get_feature_names_out()[i] for i in top_words]}')\n",
        "\n",
        "    # Print top words for gamma\n",
        "    print(\"\\nTop words for gamma:\")\n",
        "    for env_index, env_gamma in enumerate(gamma):\n",
        "        print(f\"Environment {env_index+1}:\")\n",
        "        for i, topic in enumerate(env_gamma):\n",
        "            top_words = topic.topk(num_top_words).indices\n",
        "            print(f'Topic {i+1}: {[vectorizer.get_feature_names_out()[i] for i in top_words]}')\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81JGZOe-Z9mq"
      },
      "outputs": [],
      "source": [
        "print_top_words(env_tm_model, vectorizer, num_top_words=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "def get_top_indices_values(arr, top_n=8):\n",
        "    indices = np.argsort(-arr)[:top_n]\n",
        "    values = arr[indices]\n",
        "    return indices, values\n",
        "\n",
        "def get_words(vocabulary, indices):\n",
        "    return [vocabulary[i] for i in indices]\n",
        "\n",
        "def plot_gamma_beta_heatmaps(gamma_data, beta_data, words, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Defining the color scale between 0 and 1\n",
        "    im1 = ax1.imshow(gamma_data.T, cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
        "    im2 = ax2.imshow(beta_data.T.reshape(-1, 1), cmap='hot', interpolation='nearest', vmin=0, vmax=1)\n",
        "\n",
        "    num_environments = gamma_data.shape[0]\n",
        "    environments = [f'Environment {i}' for i in range(num_environments)]\n",
        "\n",
        "    # Settings for gamma heatmap\n",
        "    ax1.set_yticks(np.arange(len(words)))\n",
        "    ax1.set_xticks(np.arange(num_environments))\n",
        "    ax1.set_yticklabels(words)\n",
        "    ax1.set_xticklabels(environments)\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    cbar1 = fig.colorbar(im1, ax=ax1)\n",
        "    cbar1.ax.set_ylabel(\"Normalized Gamma\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Settings for beta grid\n",
        "    ax2.set_yticks(np.arange(len(words)))\n",
        "    ax2.set_xticks([0])\n",
        "    ax2.set_yticklabels(words)\n",
        "    ax2.set_xticklabels(['Beta'])\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    cbar2 = fig.colorbar(im2, ax=ax2)\n",
        "    cbar2.ax.set_ylabel(\"Normalized Beta\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "    ax1.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_topic(env_tm_model, vocabulary, topic_index, top_n=8):\n",
        "    # Normalize the entire beta array for the specific topic\n",
        "    beta_values = normalize(env_tm_model.beta[topic_index, :].cpu().detach().numpy())\n",
        "\n",
        "    # Normalize the entire gamma arrays for the specific topic in all environments\n",
        "    num_environments = env_tm_model.gamma.shape[0]\n",
        "    gamma_values = [normalize(env_tm_model.gamma[i, topic_index, :].cpu().detach().numpy()) for i in range(num_environments)]\n",
        "\n",
        "    # Get the top beta indices and values\n",
        "    beta_indices, _ = get_top_indices_values(beta_values, top_n)\n",
        "\n",
        "    for env_index, gamma_value in enumerate(gamma_values):\n",
        "        # Get the top gamma indices and values\n",
        "        gamma_indices, _ = get_top_indices_values(gamma_value, top_n)\n",
        "\n",
        "        # Get the corresponding words from the vocabulary\n",
        "        gamma_words = get_words(vocabulary, gamma_indices)\n",
        "        beta_words = get_words(vocabulary, beta_indices)\n",
        "\n",
        "        # Print the top words\n",
        "        print(f\"Top words in gamma environment {env_index}:\", gamma_words)\n",
        "        print(\"Top words in beta:               \", beta_words)\n",
        "\n",
        "        # Get the gamma and beta values for top words\n",
        "        gamma_values_top_words = [gamma_values[i][gamma_indices] for i in range(num_environments)]\n",
        "        beta_values_top_words = beta_values[gamma_indices]\n",
        "\n",
        "        # Plot the heatmaps\n",
        "        plot_gamma_beta_heatmaps(np.array(gamma_values_top_words), beta_values_top_words, gamma_words, f\"Environment {env_index}: Top Words\")\n",
        "\n",
        "    # Gamma and Beta values for top words in beta\n",
        "    gamma_values_beta = [gamma_values[i][beta_indices] for i in range(num_environments)]\n",
        "    beta_values_beta = beta_values[beta_indices]\n",
        "    beta_words = get_words(vocabulary, beta_indices)\n",
        "\n",
        "    # Plot the heatmaps for the top words in beta\n",
        "    plot_gamma_beta_heatmaps(np.array(gamma_values_beta), beta_values_beta, beta_words, \"Beta: Top Words\")\n",
        "\n",
        "\n",
        "vocabulary = list(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Analyzing topic 4 with 8 top words\n",
        "analyze_topic(env_tm_model, vocabulary, topic_index=4, top_n=8)\n"
      ],
      "metadata": {
        "id": "3Cg7ry_nZMvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vAMVIhJ3k7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}